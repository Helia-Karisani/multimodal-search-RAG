{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4: Multimodal Retrieval Augmented Generation (MM-RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this lesson you'll learn how to leverage Weaviate and Google Gemini to carry out a simple multimodal RAG workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this classroom, the libraries have been already installed for you.\n",
    "* If you would like to run this code on your own machine, you need to install the following:\n",
    "```\n",
    "    !pip install -U weaviate-client\n",
    "    !pip install google-generativeai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Load environment variables and API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# get necessary APIs\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "EMBEDDING_API_KEY = os.getenv(\"EMBEDDING_API_KEY\")\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: learn more about [GOOGLE_API_KEY](https://ai.google.dev/) to run it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /home/jovyan/.cache/weaviate-embedded: process ID 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2025-11-05T16:54:58Z\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2025-11-05T16:54:58Z\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2025-11-05T16:54:58Z\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50050\",\"time\":\"2025-11-05T16:54:58Z\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2025-11-05T16:54:58Z\"}\n",
      "/usr/local/lib/python3.11/site-packages/weaviate/warnings.py:130: DeprecationWarning: Dep005: You are using weaviate-client version 4.5.4. The latest version is 4.17.0.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"telemetry_push\",\"level\":\"info\",\"msg\":\"telemetry started\",\"payload\":\"\\u0026{MachineID:53c090dc-8416-4d0d-bcbc-99f22f32f371 Type:INIT Version:1.24.21 NumObjects:0 OS:linux Arch:amd64 UsedModules:[]}\",\"time\":\"2025-11-05T16:54:59Z\"}\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_embedded(\n",
    "    version=\"1.24.21\",\n",
    "    environment_variables={\n",
    "        \"ENABLE_MODULES\": \"backup-filesystem,multi2vec-palm\",\n",
    "        \"BACKUP_FILESYSTEM_PATH\": \"/home/jovyan/work/L4/backups\", # where prevectorized data are\n",
    "    },\n",
    "    headers={\n",
    "        \"X-PALM-Api-Key\": EMBEDDING_API_KEY,\n",
    "    }\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore 13k+ prevectorized resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"try_restore\",\"backend\":\"filesystem\",\"backup_id\":\"resources-img-and-vid\",\"level\":\"info\",\"msg\":\"\",\"time\":\"2025-11-05T16:56:10Z\",\"took\":1282117}\n",
      "{\"action\":\"hnsw_prefill_cache_async\",\"level\":\"info\",\"msg\":\"not waiting for vector cache prefill, running in background\",\"time\":\"2025-11-05T16:56:11Z\",\"wait_for_cache_prefill\":false}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard resources_YR4XdMe3ODX3 in 4.969316ms\",\"time\":\"2025-11-05T16:56:11Z\"}\n",
      "{\"action\":\"restore\",\"backup_id\":\"resources-img-and-vid\",\"class\":\"Resources\",\"level\":\"info\",\"msg\":\"successfully restored\",\"time\":\"2025-11-05T16:56:11Z\"}\n",
      "{\"action\":\"restore\",\"backup_id\":\"resources-img-and-vid\",\"level\":\"info\",\"msg\":\"backup restored successfully\",\"time\":\"2025-11-05T16:56:11Z\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":16062,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2025-11-05T16:56:11Z\",\"took\":150554948}\n",
      "{\"action\":\"restore\",\"backup_id\":\"resources-img-and-vid\",\"level\":\"info\",\"msg\":\"coordinator: backup restored successfully\",\"time\":\"2025-11-05T16:56:12Z\"}\n"
     ]
    }
   ],
   "source": [
    "client.collections.delete(\"Resources\")\n",
    "\n",
    "client.backup.restore(\n",
    "    backup_id=\"resources-img-and-vid\",\n",
    "    include_collections=\"Resources\", # where we load new dataset\n",
    "    backend=\"filesystem\"\n",
    ")\n",
    "\n",
    "# It can take a few seconds for the \"Resources\" collection to be ready.\n",
    "# We add 5 seconds of sleep to make sure it is ready for the next cells to use.\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview data count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image count: 13394\n",
      "video count: 200\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.aggregate import GroupByAggregate\n",
    "\n",
    "resources = client.collections.get(\"Resources\")\n",
    "\n",
    "response = resources.aggregate.over_all(\n",
    "    group_by=GroupByAggregate(prop=\"mediaType\") # count all object and group them based on media type\n",
    ")\n",
    "\n",
    "# print rounds names and the count for each\n",
    "for group in response.groups:\n",
    "    print(f\"{group.grouped_by.value} count: {group.total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 ‚Äì Retrieve content from the database with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "# first step of running full multimodal RAG\n",
    "from IPython.display import Image\n",
    "from weaviate.classes.query import Filter\n",
    "\n",
    "# given a query we want to retrieve an image\n",
    "def retrieve_image(query):\n",
    "    resources = client.collections.get(\"Resources\")\n",
    "# ============\n",
    "    response = resources.query.near_text(\n",
    "        query=query,\n",
    "        # only images to pass to the vision model later\n",
    "        filters=Filter.by_property(\"mediaType\").equal(\"image\"), # only return image objects\n",
    "        # only interested in the path to the onject and return just one object\n",
    "        return_properties=[\"path\"],\n",
    "        # return just one object\n",
    "        limit = 1,\n",
    "    )\n",
    "# ============\n",
    "    # grab the first object\n",
    "    result = response.objects[0].properties\n",
    "    return result[\"path\"] # return  the image URL that matches our query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run image retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Try with different queries to retreive an image\n",
    "img_path = retrieve_image(\"fishing with my buddies\")\n",
    "display(Image(img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access Files and Helper Functions:</b> To access the files for this notebook, 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Generate a description of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "# set API key for generative model\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "\n",
    "# Set the Vision model key\n",
    "genai.configure(\n",
    "        api_key=GOOGLE_API_KEY,\n",
    "        transport=\"rest\",\n",
    "        client_options=ClientOptions(\n",
    "            api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Late 2024, the model `'gemini-pro-vision'` (originally shown in the video's notebook) has been replaced with `'gemini-1.5-flash'` for updates.\n",
    "Additionally, due to this change, the parameter `stream=True` has now been set to `stream=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import textwrap\n",
    "import PIL.Image\n",
    "from IPython.display import Markdown, Image\n",
    "\n",
    "# to convert output to markdown\n",
    "def to_markdown(text):\n",
    "    text = text.replace(\"‚Ä¢\", \"  *\")\n",
    "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))\n",
    "\n",
    "# calling the function that given the image path and a prompt returns a noce description of the image\n",
    "def call_LMM(image_path: str, prompt: str) -> str:\n",
    "    img = PIL.Image.open(image_path)\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content([prompt, img], stream=False)\n",
    "    response.resolve()\n",
    "\n",
    "    return to_markdown(response.text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#cceecc; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> üö® <b>Different Run Results:</b> The output generated by AI models can vary with each execution due to their probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run vision request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# path from step 1 and description\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m call_LMM(\u001b[43mimg_path\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease describe this image in detail.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_path' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 80.04%, threshold set to 80.00%\",\"path\":\"/home/jovyan/.local/share/weaviate\",\"time\":\"2025-11-05T17:07:08Z\"}\n",
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 80.75%, threshold set to 80.00%\",\"path\":\"/home/jovyan/.local/share/weaviate\",\"time\":\"2025-11-05T17:07:38Z\"}\n",
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 82.45%, threshold set to 80.00%\",\"path\":\"/home/jovyan/.local/share/weaviate\",\"time\":\"2025-11-05T17:09:38Z\"}\n"
     ]
    }
   ],
   "source": [
    "# path from step 1 and description\n",
    "call_LMM(img_path, \"Please describe this image in detail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Please be aware that the output from the previous cell may differ from what is shown in the video. This variation is normal and should not cause concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "# combine all together\n",
    "# create a function where the first step will be to call the retrieve image function \n",
    "\n",
    "def mm_rag(query):\n",
    "    # Step 1 - retrieve an image ‚Äì Weaviate\n",
    "    SOURCE_IMAGE = retrieve_image(query)\n",
    "    # the output will be saved inside SOURCE_IMAGE variable\n",
    "    display(Image(SOURCE_IMAGE)) \n",
    "#===========\n",
    "    \n",
    "    # Step 2 - generate a description - GPT4\n",
    "    # call LLM with source image from previous step and the prompt\n",
    "    # and that should return the description\n",
    "    description = call_LMM(SOURCE_IMAGE, \"Please describe this image in detail.\")\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Call mm_rag function\n",
    "mm_rag(\"paragliding through the mountains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Remember to close the weaviate instance\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "\n",
    "Run the cells above selecting another image from the database and generate a description for it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
